{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "# 激活函数\n",
    "class ActivationBase(ABC):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "    def __call__(self, z):\n",
    "        if z.dim == 1:\n",
    "            z = z.reshape(1, -1)\n",
    "        return self.forward(z)\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, z):\n",
    "        \"\"\"前向传播，通过激活函数得到 a\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def grad(self, x, **kwargs):\n",
    "        \"\"\"反向传播，获得梯度\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class ReLU(ActivationBase):\n",
    "    \"\"\"整流线性单元\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'ReLU'\n",
    "\n",
    "    def forward(self, z):\n",
    "        return np.clip(z, 0, np.inf)\n",
    "\n",
    "    def grad(self, x, **kwargs):\n",
    "        return (x > 0).astype(int)\n",
    "\n",
    "\n",
    "class Sigmoid(ActivationBase):\n",
    "    \"\"\"sigmoid 激活函数\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'Sigmoid'\n",
    "\n",
    "    def forward(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def grad(self, x, **kwargs):\n",
    "        return self.forward(x) * (1 - self.forward(x))\n",
    "\n",
    "\n",
    "class Tanh(ActivationBase):\n",
    "    \"\"\"双曲正弦函数\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'Tanh'\n",
    "\n",
    "    def forward(self, z):\n",
    "        return np.tanh(z)\n",
    "\n",
    "    def grad(self, x, **kwargs):\n",
    "        return 1 - np.tanh(x) ** 2\n",
    "\n",
    "\n",
    "class Affine(ActivationBase):\n",
    "    \"\"\"affine 激活函数，即仿射变换。输出 slope*z + intercept。当 slope=1 且 intercept=0 表示不做变换\"\"\"\n",
    "\n",
    "    def __init__(self, slope=1, intercept=0):\n",
    "        self.slope = slope\n",
    "        self.intercept = intercept\n",
    "        super().__init__()\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'Affine(slope={self.slope}, intercept={self.intercept})'\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.slope * z + self.intercept\n",
    "\n",
    "    def grad(self, x, **kwargs):\n",
    "        return self.slope * np.ones_like(x)\n",
    "\n",
    "\n",
    "class ActivationInitializer:\n",
    "    def __init__(self, acti_name='sigmoid'):\n",
    "        self.acti_name = acti_name\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        if self.acti_name.lower() == 'sigmoid':\n",
    "            acti_fn = Sigmoid()\n",
    "        elif self.acti_name.lower() == 'relu':\n",
    "            acti_fn = ReLU()\n",
    "        elif 'affine' in self.acti_name.lower():\n",
    "            r = r'affine\\(slope(.*), intercept=(.*)\\)'\n",
    "            slope, intercept = re.match(r, self.acti_name.lower()).groups()\n",
    "            acti_fn = Affine(float(slope), float(intercept))\n",
    "        return acti_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# 输出单元\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return e_x / e_x.sum(axis=-1, keepdims=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# 定义权重初始化方法\n",
    "class std_normal:\n",
    "    \"\"\"标准正态初始化\"\"\"\n",
    "\n",
    "    def __init__(self, gain=0.01):\n",
    "        self.gain = gain\n",
    "\n",
    "    def __call__(self, weight_shape):\n",
    "        return self.gain * np.random.randn(*weight_shape)\n",
    "\n",
    "\n",
    "class he_uniform:\n",
    "    \"\"\"He 初始化，通过 Uniform(-b, b) 初始化权重矩阵 W，这里的 b=sqrt(6 / n_in)\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, weight_shape):\n",
    "        n_in, n_out = weight_shape\n",
    "        b = np.sqrt(6 / n_in)\n",
    "        return np.random.uniform(-b, b, size=weight_shape)\n",
    "\n",
    "\n",
    "class WeightInitializer:\n",
    "    def __init__(self, mode='he_uniform'):\n",
    "        self.mode = mode\n",
    "        r = r'([a-zA-z]*)=([^,]*)'\n",
    "        mode_str = self.mode.lower()\n",
    "        kwargs = dict([(i, eval(j)) for (i, j) in re.findall(r, mode_str)])\n",
    "\n",
    "        if 'std_normal' in mode_str:\n",
    "            self.init_fn = std_normal(**kwargs)\n",
    "        elif 'he_uniform' in mode_str:\n",
    "            self.init_fn = he_uniform(**kwargs)\n",
    "\n",
    "    def __call__(self, weight_shape):\n",
    "        W = self.init_fn(weight_shape)\n",
    "        return W"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# 定义优化 - sgd\n",
    "class OptimizerBase(ABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, params, params_grad, params_name):\n",
    "        \"\"\"\n",
    "        参数说明\n",
    "        :param params: 待更新参数，如权重矩阵 w；\n",
    "        :param params_grad: 待更新参数的梯度；\n",
    "        :param params_name: 待更新参数的名称\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return self.update(params, params_grad, params_name)\n",
    "\n",
    "    @abstractmethod\n",
    "    def update(self, params, params_grad, params_name):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class SGD(OptimizerBase):\n",
    "    \"\"\"sgd 优化方法\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'SGD(lr={self.hyperparams[\"lr\"]})'\n",
    "\n",
    "    def update(self, params, params_grad, params_name):\n",
    "        update_value = self.lr * params_grad\n",
    "        return params - update_value\n",
    "\n",
    "    @property\n",
    "    def hyperparams(self):\n",
    "        return {\n",
    "            'op': 'SGD',\n",
    "            'lr': self.lr\n",
    "        }\n",
    "\n",
    "\n",
    "class OptimizerInitializer(ABC):\n",
    "    def __init__(self, opti_name=\"sgd\"):\n",
    "        self.opti_name = opti_name\n",
    "\n",
    "    def __call__(self):\n",
    "        r = r\"([a-zA-Z]*)=([^,)]*)\"\n",
    "        opti_str = self.opti_name.lower()\n",
    "        kwargs = dict([(i, eval(j)) for (i, j) in re.findall(r, opti_str)])\n",
    "\n",
    "        if \"sgd\" in opti_str:\n",
    "            optimizer = SGD(**kwargs)\n",
    "\n",
    "        return optimizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# 定义网络层的框架\n",
    "class LayerBase(ABC):\n",
    "    def __init__(self, optimizer=None):\n",
    "        self.X = []  # 网络层输入\n",
    "        self.gradients = {}  # 网络层待梯度更新变量\n",
    "        self.params = {}  # 网络层参数变量\n",
    "        self.acti_fn = None  # 网络层激活函数\n",
    "        self.optimizer = OptimizerInitializer(optimizer)()  # 网络层优化方法\n",
    "\n",
    "    @abstractmethod\n",
    "    def _init_params(self, **kwargs):\n",
    "        \"\"\"初始化参数\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, X, **kwargs):\n",
    "        \"\"\"前向传播\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self, out, **kwargs):\n",
    "        \"\"\"反向传播\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def flush_gradients(self):\n",
    "        \"\"\"重置更新参数列表\"\"\"\n",
    "        self.X = []\n",
    "        for k, v in self.gradients.items():\n",
    "            self.gradients[k] = np.zeros_like(v)\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"更新参数\"\"\"\n",
    "        for k, v in self.gradients.items():\n",
    "            if k in self.params:\n",
    "                self.params[k] = self.optimizer(self.params[k], v, k)\n",
    "\n",
    "\n",
    "class FullyConnected(LayerBase):\n",
    "    \"\"\"定义全链接层，实现 a=g(x*W+b)，前向传播输入 x，返回 a；反向传播输入\"\"\"\n",
    "\n",
    "    def __init__(self, n_out, acti_fn, init_w, optimizer=None):\n",
    "        \"\"\"\n",
    "        参数说明\n",
    "        :param n_out: 隐藏层输出的维数\n",
    "        :param acti_fn: 激活函数，str 型\n",
    "        :param init_w: 权重初始化方法，str 型\n",
    "        :param optimizer: 优化方法\n",
    "        \"\"\"\n",
    "        super().__init__(optimizer)\n",
    "        self.n_in = None  # 隐藏层输入维数，int\n",
    "        self.n_out = n_out  # 隐藏层输出维数，int\n",
    "        self.acti_fn = ActivationInitializer(acti_fn)()\n",
    "        self.init_w = init_w\n",
    "        self.init_weights = WeightInitializer(mode=init_w)\n",
    "        self.is_initialized = False  # 是否初始化，bool\n",
    "\n",
    "    def _init_params(self):\n",
    "        b = np.zeros((1, self.n_out))\n",
    "        W = self.init_weights((self.n_in, self.n_out))\n",
    "        self.params = {'W': W, 'b': b}\n",
    "        self.gradients = {'W': np.zeros_like(W), 'b': np.zeros_like(b)}\n",
    "        self.is_initialized = True\n",
    "\n",
    "    def forward(self, X, retain_derived=True):\n",
    "        \"\"\"全连接网络的前向传播，原理见上文 反向传播算法 部分。\n",
    "        参数说明：\n",
    "        X：输入数组，为（n_samples, n_in），float 型\n",
    "        retain_derived：是否保留中间变量，以便反向传播时再次使用，bool 型\n",
    "        \"\"\"\n",
    "        if not self.is_initialized:  # 如果参数未初始化，先初始化参数\n",
    "            self.n_in = X.shape[1]\n",
    "            self._init_params()\n",
    "        W = self.params[\"W\"]\n",
    "        b = self.params[\"b\"]\n",
    "\n",
    "        z = X @ W + b\n",
    "        a = self.acti_fn.forward(z)\n",
    "\n",
    "        if retain_derived:\n",
    "            self.X.append(X)\n",
    "        return a\n",
    "\n",
    "    def backward(self, dLda, retain_grads=True):\n",
    "        \"\"\"\n",
    "        全连接网络的反向传播，原理见上文 反向传播算法 部分。\n",
    "        参数说明：\n",
    "        dLda：关于损失的梯度，为（n_samples, n_out），float 型\n",
    "        retain_grads：是否计算中间变量的参数梯度，bool 型\n",
    "        \"\"\"\n",
    "        if not isinstance(dLda, list):\n",
    "            dLda = [dLda]\n",
    "        dX = []\n",
    "        X = self.X\n",
    "        for da, x in zip(dLda, X):\n",
    "            dx, dw, db = self._bwd(da, x)\n",
    "            dX.append(dx)\n",
    "            if retain_grads:\n",
    "                self.gradients[\"W\"] += dw\n",
    "                self.gradients[\"b\"] += db\n",
    "        return dX[0] if len(X) == 1 else dX\n",
    "\n",
    "    def _bwd(self, dLda, X):\n",
    "        W = self.params[\"W\"]\n",
    "        b = self.params[\"b\"]\n",
    "        Z = X @ W + b\n",
    "        dZ = dLda * self.acti_fn.grad(Z)\n",
    "        dX = dZ @ W.T\n",
    "        dW = X.T @ dZ\n",
    "        db = dZ.sum(axis=0, keepdims=True)\n",
    "        return dX, dW, db\n",
    "\n",
    "    @property\n",
    "    def hyperparams(self):\n",
    "        return {\n",
    "            \"layer\": \"FullyConnected\",\n",
    "            \"init_w\": self.init_w,\n",
    "            \"n_in\": self.n_in,\n",
    "            \"n_out\": self.n_out,\n",
    "            \"acti_fn\": str(self.acti_fn),\n",
    "            \"optimizer\": {\n",
    "                \"hyperparams\": self.optimizer.hyperparams,\n",
    "            },\n",
    "            \"components\": {\n",
    "                k: v for k, v in self.params.items()\n",
    "            }\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class Softmax(LayerBase):\n",
    "    \"\"\"\n",
    "    定义 Softmax 层\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim=-1, optimizer=None):\n",
    "        super().__init__(optimizer)\n",
    "        self.dim = dim\n",
    "        self.n_in = None\n",
    "        self.is_initialized = False\n",
    "\n",
    "    def _init_params(self):\n",
    "        self.params = {}\n",
    "        self.gradients = {}\n",
    "        self.is_initialized = True\n",
    "\n",
    "    def forward(self, X, retain_derived=True):\n",
    "        \"\"\"\n",
    "        Softmax 的前向传播， 原理见上文 代价函数 部分。\n",
    "        \"\"\"\n",
    "        if not self.is_initialized:\n",
    "            self.n_in = X.shape[1]\n",
    "            self._init_params()\n",
    "        Y = self._fwd(X)\n",
    "        if retain_derived:\n",
    "            self.X.append(X)\n",
    "        return Y\n",
    "\n",
    "    def _fwd(self, X):\n",
    "        e_X = np.exp(X - np.max(X, axis=self.dim, keepdims=True))\n",
    "        return e_X / e_X.sum(axis=self.dim, keepdims=True)\n",
    "\n",
    "    def backward(self, dLdy):\n",
    "        \"\"\"\n",
    "        Softmax 的反向传播，原理见上文 代价函数 部分。\n",
    "        \"\"\"\n",
    "        if not isinstance(dLdy, list):\n",
    "            dLdy = [dLdy]\n",
    "        dX = []\n",
    "        X = self.X\n",
    "        for dy, x in zip(dLdy, X):\n",
    "            dx = self._bwd(dy, x)\n",
    "            dX.append(dx)\n",
    "        return dX[0] if len(X) == 1 else dX\n",
    "\n",
    "    def _bwd(self, dLdy, X):\n",
    "        dX = []\n",
    "        for dy, x in zip(dLdy, X):\n",
    "            dxi = []\n",
    "            for dyi, xi in zip(*np.atleast_2d(dy, x)):\n",
    "                yi = self._fwd(xi.reshape(1, -1)).reshape(-1, 1)\n",
    "                dyidxi = np.diagflat(yi) - yi @ yi.T\n",
    "                dxi.append(dyi @ dyidxi)\n",
    "            dX.append(dxi)\n",
    "        return np.array(dX).reshape(*X.shape)\n",
    "\n",
    "    @property\n",
    "    def hyperparams(self):\n",
    "        return {\n",
    "            \"layer\": \"SoftmaxLayer\",\n",
    "            \"n_in\": self.n_in,\n",
    "            \"n_out\": self.n_in,\n",
    "            \"optimizer\": {\n",
    "                \"hyperparams\": self.optimizer.hyperparams,\n",
    "            },\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# 定义代价函数\n",
    "class ObjectiveBase(ABC):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @abstractmethod\n",
    "    def loss(self, y_true, y_pred):\n",
    "        \"\"\"计算损失\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def grad(self, y_true, y_pred, **kwargs):\n",
    "        \"\"\"计算代价函数的梯度\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class SquaredError(ObjectiveBase):\n",
    "    \"\"\"二次代价函数。\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        return self.loss(y_true, y_pred)\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"SquaredError\"\n",
    "\n",
    "    @staticmethod\n",
    "    def loss(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        参数说明：\n",
    "        y_true：训练的 n 个样本的真实值， 形状为 (n,m) 数组；\n",
    "        y_pred：训练的 n 个样本的预测值， 形状为 (n,m) 数组；\n",
    "        \"\"\"\n",
    "        (n, _) = y_true.shape\n",
    "        return 0.5 * np.linalg.norm(y_pred - y_true) ** 2 / n\n",
    "\n",
    "    @staticmethod\n",
    "    def grad(y_true, y_pred, z, acti_fn):\n",
    "        (n, _) = y_true.shape\n",
    "        return (y_pred - y_true) * acti_fn.grad(z) / n\n",
    "\n",
    "\n",
    "class CrossEntropy(ObjectiveBase):\n",
    "    \"\"\"\n",
    "    交叉熵代价函数。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        return self.loss(y_true, y_pred)\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"CrossEntropy\"\n",
    "\n",
    "    @staticmethod\n",
    "    def loss(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        参数说明：\n",
    "        y_true：训练的 n 个样本的真实值， 要求形状为 (n,m) 二进制（每个样本均为 one-hot 编码）；\n",
    "        y_pred：训练的 n 个样本的预测值， 形状为 (n,m)；\n",
    "        \"\"\"\n",
    "        (n, _) = y_true.shape\n",
    "        eps = np.finfo(float).eps  # 防止 np.log(0)\n",
    "        cross_entropy = -np.sum(y_true * np.log(y_pred + eps)) / n\n",
    "        return cross_entropy\n",
    "\n",
    "    @staticmethod\n",
    "    def grad(y_true, y_pred):\n",
    "        (n, _) = y_true.shape\n",
    "        grad = (y_pred - y_true) / n\n",
    "        return grad"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# 定义深度前馈网络\n",
    "def minibatch(X, batchsize=256, shuffle=True):\n",
    "    \"\"\"函数作用：将数据集分割成 batch， 基于 mini batch 训练，具体可见第 8 章。\"\"\"\n",
    "    N = X.shape[0]\n",
    "    idx = np.arange(N)\n",
    "    n_batches = int(np.ceil(N / batchsize))\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx)\n",
    "\n",
    "    def mb_generator():\n",
    "        for i in range(n_batches):\n",
    "            yield idx[i * batchsize: (i + 1) * batchsize]\n",
    "\n",
    "    return mb_generator(), n_batches\n",
    "\n",
    "\n",
    "class DFN(object):\n",
    "    def __init__(\n",
    "            self,\n",
    "            hidden_dims_1=None,\n",
    "            hidden_dims_2=None,\n",
    "            optimizer=\"sgd(lr=0.01)\",\n",
    "            init_w=\"std_normal\",\n",
    "            loss=CrossEntropy()):\n",
    "        self.optimizer = optimizer\n",
    "        self.init_w = init_w\n",
    "        self.loss = loss\n",
    "        self.hidden_dims_1 = hidden_dims_1\n",
    "        self.hidden_dims_2 = hidden_dims_2\n",
    "        self.is_initialized = False\n",
    "\n",
    "    def _set_params(self):\n",
    "        \"\"\"\n",
    "        函数作用：模型初始化\n",
    "        FC1 -> Sigmoid -> FC2 -> Softmax\n",
    "        \"\"\"\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers[\"FC1\"] = FullyConnected(\n",
    "            n_out=self.hidden_dims_1,\n",
    "            acti_fn=\"sigmoid\",\n",
    "            init_w=self.init_w,\n",
    "            optimizer=self.optimizer\n",
    "        )\n",
    "        self.layers[\"FC2\"] = FullyConnected(\n",
    "            n_out=self.hidden_dims_2,\n",
    "            acti_fn=\"affine(slope=1, intercept=0)\",\n",
    "            init_w=self.init_w,\n",
    "            optimizer=self.optimizer\n",
    "        )\n",
    "        self.is_initialized = True\n",
    "\n",
    "    def forward(self, X_train):\n",
    "        Xs = {}\n",
    "        out = X_train\n",
    "        for k, v in self.layers.items():\n",
    "            Xs[k] = out\n",
    "            out = v.forward(out)\n",
    "        return out, Xs\n",
    "\n",
    "    def backward(self, grad):\n",
    "        dXs = {}\n",
    "        out = grad\n",
    "        for k, v in reversed(list(self.layers.items())):\n",
    "            dXs[k] = out\n",
    "            out = v.backward(out)\n",
    "        return out, dXs\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        函数作用：梯度更新\n",
    "        \"\"\"\n",
    "        for k, v in reversed(list(self.layers.items())):\n",
    "            v.update()\n",
    "        self.flush_gradients()\n",
    "\n",
    "    def flush_gradients(self, curr_loss=None):\n",
    "        \"\"\"\n",
    "        函数作用：更新后重置梯度\n",
    "        \"\"\"\n",
    "        for k, v in self.layers.items():\n",
    "            v.flush_gradients()\n",
    "\n",
    "    def fit(self, X_train, y_train, n_epochs=20, batch_size=64, verbose=False):\n",
    "        \"\"\"\n",
    "        参数说明：\n",
    "        X_train：训练数据\n",
    "        y_train：训练数据标签\n",
    "        n_epochs：epoch 次数\n",
    "        batch_size：每次 epoch 的 batch size\n",
    "        verbose：是否每个 batch 输出损失\n",
    "        \"\"\"\n",
    "        self.verbose = verbose\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        if not self.is_initialized:\n",
    "            self.n_features = X_train.shape[1]\n",
    "            self._set_params()\n",
    "        prev_loss = np.inf\n",
    "\n",
    "        for i in range(n_epochs):\n",
    "            loss, epoch_start = 0.0, time.time()\n",
    "            batch_generator, n_batch = minibatch(X_train, self.batch_size, shuffle=True)\n",
    "            for j, batch_idx in enumerate(batch_generator):\n",
    "                batch_len, batch_start = len(batch_idx), time.time()\n",
    "                X_batch, y_batch = X_train[batch_idx], y_train[batch_idx]\n",
    "                out, _ = self.forward(X_batch)\n",
    "                y_pred_batch = softmax(out)\n",
    "                batch_loss = self.loss(y_batch, y_pred_batch)\n",
    "                grad = self.loss.grad(y_batch, y_pred_batch)\n",
    "                _, _ = self.backward(grad)\n",
    "                self.update()\n",
    "                loss += batch_loss\n",
    "                if self.verbose:\n",
    "                    fstr = \"\\t[Batch {}/{}] Train loss: {:.3f} ({:.1f}s/batch)\"\n",
    "                    print(fstr.format(j + 1, n_batch, batch_loss, time.time() - batch_start))\n",
    "\n",
    "            loss /= n_batch\n",
    "            fstr = \"[Epoch {}] Avg. loss: {:.3f} Delta: {:.3f} ({:.2f}m/epoch)\"\n",
    "            print(fstr.format(i + 1, loss, prev_loss - loss, (time.time() - epoch_start) / 60.0))\n",
    "            prev_loss = loss\n",
    "\n",
    "    def evaluate(self, X_test, y_test, batch_size=128):\n",
    "        acc = 0.0\n",
    "        batch_generator, n_batch = minibatch(X_test, batch_size, shuffle=True)\n",
    "        for j, batch_idx in enumerate(batch_generator):\n",
    "            batch_len, batch_start = len(batch_idx), time.time()\n",
    "            X_batch, y_batch = X_test[batch_idx], y_test[batch_idx]\n",
    "            y_pred_batch, _ = self.forward(X_batch)\n",
    "            y_pred_batch = np.argmax(y_pred_batch, axis=1)\n",
    "            y_batch = np.argmax(y_batch, axis=1)\n",
    "            acc += np.sum(y_pred_batch == y_batch)\n",
    "        return acc / X_test.shape[0]\n",
    "\n",
    "    @property\n",
    "    def hyperparams(self):\n",
    "        return {\n",
    "            \"init_w\": self.init_w,\n",
    "            \"loss\": str(self.loss),\n",
    "            \"optimizer\": self.optimizer,\n",
    "            \"hidden_dims_1\": self.hidden_dims_1,\n",
    "            \"hidden_dims_2\": self.hidden_dims_2,\n",
    "            \"components\": {k: v.params for k, v in self.layers.items()}\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_data(path=\"../data/mnist/mnist.npz\"):\n",
    "    f = np.load(path)\n",
    "    X_train, y_train = f['x_train'], f['y_train']\n",
    "    X_test, y_test = f['x_test'], f['y_test']\n",
    "    f.close()\n",
    "    return (X_train, y_train), (X_test, y_test)\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = load_data()\n",
    "y_train = np.eye(10)[y_train.astype(int)]\n",
    "y_test = np.eye(10)[y_test.astype(int)]\n",
    "X_train = X_train.reshape(-1, X_train.shape[1]*X_train.shape[2]).astype('float32')\n",
    "X_test = X_test.reshape(-1, X_test.shape[1]*X_test.shape[2]).astype('float32')\n",
    "print(X_train.shape, y_train.shape)\n",
    "N = 20000 # 取 20000 条数据用以训练\n",
    "indices = np.random.permutation(range(X_train.shape[0]))[:N]\n",
    "X_train, y_train = X_train[indices], y_train[indices]\n",
    "print(X_train.shape, y_train.shape)\n",
    "X_train /= 255\n",
    "X_train = (X_train - 0.5) * 2\n",
    "X_test /= 255\n",
    "X_test = (X_test - 0.5) * 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
